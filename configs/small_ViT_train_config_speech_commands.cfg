#AI Cloud
dataset_root: /home/es.aau.dk/ha11jc/Datasets/speechcommands
labels_map: /home/es.aau.dk/ha11jc/Datasets/speechcommands/lbl_map.json
mode: multiclass


audio_config:
    feature: melspectrogram
    n_mels: 80
    num_frames: 100
    # n_fft: 511
    # win_len:
    # hop_len:
    # normalize:
    sample_rate: 16000
    min_duration: 1
exp:
    exp_name: 100_epoch_4_gpu_small_model_patch_size_16x4_ViT_batch_size_256_speech_commands
    wandb: False
    wandb_api_key: 
    proj_name: Fourier-approximation-for-local-global-Multi-Head-Attention
    entity: ce8-840
    # save_dir: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_speech_commands/model_checkpoints/
    save_dir: /home/es.aau.dk/ha11jc/VIT_speech_commands
    log_freq: 10
    val_freq: 1
    log_to_file: False
    log_to_stdout: False

hparams:
    #batch_size: 100 # use af batch size that is the power of 2 
    batch_size: 256 # use af batch size that is the power of 2, maybe try 1024 for GPU
    n_epochs: 100
    #scheduler:
     #   n_warmup: 10 # fit with number of epochs
    early_stopping_patience: 100 # end if no improvement after 5 epochs
    KWT:
        #input_res: [96, 101] #shape of FSD50K spectograms
        input_res: [80, 100] #shape of FSD50K spectograms
        #input_res: [96, 501] #shape of FSD50K spectograms #try 5 seconds instead
        #patch_res: [96, 1]  # patch shape -- try to divide into 16 frequency bins for every window, 16,4.
        patch_res: [16, 4] 
        num_classes: 200 # number of classes 
        mlp_dim: 1536 # MLP dimensions. embedding_dim to mlp_dim ratio: 1:4
        dim: 384  # number of embeddings 
        heads: 6 # number of attention heads
        depth: 12 #depth of network
        # dropout: 0.0
        # emb_dropout: 0.1
        # pre_norm: False
        # pool: mean
    optimizer:
        lr: 0.0001
        betas: !!python/tuple [0.9, 0.98]
        eps: 0.000001
        weight_decay: 0.01


