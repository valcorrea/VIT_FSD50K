#AI Cloud
tr_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/tr.csv
val_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/val.csv
eval_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/eval.csv
labels_map: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/lbl_map.json

audio_config:
    feature: melspectrogram
    # n_fft: 511
    # win_len:
    # hop_len:
    # normalize:
    sample_rate: 22050
    min_duration: 1
exp:
    exp_name: small_model_patch_size_16x4_40_epochs_ViT_batch_size_256
    wandb: True
    wandb_api_key: 
    proj_name: Fourier-approximation-for-local-global-Multi-Head-Attention
    entity: ce8-840
    save_dir: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/model_checkpoints/
    log_freq: 10
    val_freq: 1
    log_to_file: False
    log_to_stdout: False

hparams:
    #batch_size: 100 # use af batch size that is the power of 2 
    batch_size: 256 # use af batch size that is the power of 2 
    n_epochs: 40
    scheduler:
        n_warmup: 5
    early_stopping_patience: 5 # end if no improvement after 5 epochs
    KWT:
        #input_res: [96, 101] #shape of FSD50K spectograms
        input_res: [96, 100] #shape of FSD50K spectograms
        #input_res: [96, 501] #shape of FSD50K spectograms #try 5 seconds instead
        #patch_res: [96, 1]  # patch shape -- try to divide into 16 frequency bins for every window, 16,4.
        patch_res: [16, 4] 
        num_classes: 200 # number of classes 
        mlp_dim: 1536 # MLP dimensions. embedding_dim to mlp_dim ratio: 1:4
        dim: 384  # number of embeddings 
        heads: 6 # number of attention heads
        depth: 12 #depth of network
        # dropout: 0.0
        # emb_dropout: 0.1
        # pre_norm: False
        # pool: mean
    optimizer:
        lr: 0.0005
        betas: !!python/tuple [0.9, 0.98]
        eps: 0.000001
        weight_decay: 0.01


