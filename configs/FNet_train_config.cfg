#Local
tr_manifest_path: /Users/valentcorrea/git/local-global-Mul-Head-Attention/VIT_FSD50K/dataset/chunks_meta_dir/tr.csv
val_manifest_path: /Users/valentcorrea/git/local-global-Mul-Head-Attention/VIT_FSD50K/dataset/chunks_meta_dir/val.csv
eval_manifest_path: /Users/valentcorrea/git/local-global-Mul-Head-Attention/VIT_FSD50K/dataset/chunks_meta_dir/eval.csv
labels_map: /Users/valentcorrea/git/local-global-Mul-Head-Attention/VIT_FSD50K/dataset/chunks_meta_dir/lbl_map.json


# #AI Cloud
# tr_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/tr.csv
# val_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/val.csv
# eval_manifest_path: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/eval.csv
# labels_map: /home/student.aau.dk/saales20/local-global-Mul-Head-Attention/VIT_FSD50K/data_set/chunks_meta_dir/lbl_map.json

audio_config:
    feature: melspectrogram
    # n_fft: 511
    # win_len:
    # hop_len:
    # normalize:
    sample_rate: 22050
    min_duration: 1
exp:
    exp_name: 2_epochs_FNet_classifier_lightning_usingFeedForwardFromKWT
    wandb: True
    wandb_api_key: 
    proj_name: Fourier-approximation-for-local-global-Multi-Head-Attention
    entity: ce8-840
    save_dir: outputs/
    log_freq: 10
    val_freq: 1
    log_to_file: False
    log_to_stdout: False

hparams:
    batch_size: 100
    n_epochs: 2
    scheduler:
        n_warmup: 10
    early_stopping_patience: 1
    KWT:
        input_res: [96, 101] #shape of FSD50K spectograms
        patch_res: [96, 1]  # patch shape
        num_classes: 200 # numger of classes 
        mlp_dim: 256 # MLP dimensions
        dim: 64 
        heads: 1 # number of attention heads
        depth: 12 #depth of network
        # dropout: 0.0
        # emb_dropout: 0.1
        # pre_norm: False
        # pool: mean
    KWTFNet:
        input_res: [96, 101] #shape of FSD50K spectograms
        patch_res: [96, 1]  # patch shape
        num_classes: 200 # numger of classes 
        hidden_size: 64
        num_hidden_layers: 12
        heads: 1
        intermediate_size: 256
        pool: cls
        channels: 1
        dim_head: 64
        hidden_dropout_prob: 0.0
        emb_dropout: 0.0
        pre_norm: True
    optimizer:
        lr: 0.0005
        betas: !!python/tuple [0.9, 0.98]
        eps: 0.000001
        weight_decay: 0.01


